{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Turker\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import sklearn\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble,calibration\n",
    "\n",
    "#from sklearn.cross_validation import KFold\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#import pandas, numpy, textblob, string\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train = pd.read_csv(r'training_matrix.csv')\n",
    "valid = pd.read_csv(r'validating_matrix.csv')\n",
    "test = pd.read_csv(r'testing_matrix.csv')\n",
    "\n",
    "train.loc[train.label == 1, 'label'] = 0\n",
    "train.loc[train.label == -1, 'label'] = 1\n",
    "\n",
    "valid.loc[valid.label == 1, 'label'] = 0\n",
    "valid.loc[valid.label == -1, 'label'] = 1\n",
    "\n",
    "test.loc[test.label == 1, 'label'] = 0\n",
    "test.loc[test.label == -1, 'label'] = 1\n",
    "\n",
    "\n",
    "train['char_count'] = train['review'].apply(len)\n",
    "train['word_count'] = train['review'].apply(lambda x: len(x.split()))\n",
    "train['word_density'] = train['char_count'] / (train['word_count']+1)\n",
    "train['punctuation_count'] = train['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "train['title_word_count'] = train['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "train['upper_case_word_count'] = train['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "valid['char_count'] = valid['review'].apply(len)\n",
    "valid['word_count'] = valid['review'].apply(lambda x: len(x.split()))\n",
    "valid['word_density'] = valid['char_count'] / (valid['word_count']+1)\n",
    "valid['punctuation_count'] = valid['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "valid['title_word_count'] = valid['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "valid['upper_case_word_count'] = valid['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "test['char_count'] = test['review'].apply(len)\n",
    "test['word_count'] = test['review'].apply(lambda x: len(x.split()))\n",
    "test['word_density'] = test['char_count'] / (test['word_count']+1)\n",
    "test['punctuation_count'] = test['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "test['title_word_count'] = test['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "test['upper_case_word_count'] = test['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Day is  Thursday\n",
      "fake is  764\n",
      "real is  755\n",
      "tot is  1519\n",
      "Ratio is  50.29624753127058\n",
      "len is  1519\n",
      "***********Day is  Tuesday\n",
      "fake is  800\n",
      "real is  788\n",
      "tot is  1588\n",
      "Ratio is  50.377833753148614\n",
      "len is  1588\n",
      "***********Day is  Friday\n",
      "fake is  758\n",
      "real is  701\n",
      "tot is  1459\n",
      "Ratio is  51.953392734749826\n",
      "len is  1459\n",
      "***********Day is  Saturday\n",
      "fake is  756\n",
      "real is  744\n",
      "tot is  1500\n",
      "Ratio is  50.4\n",
      "len is  1500\n",
      "***********Day is  Monday\n",
      "fake is  858\n",
      "real is  913\n",
      "tot is  1771\n",
      "Ratio is  48.4472049689441\n",
      "len is  1771\n",
      "***********Day is  Sunday\n",
      "fake is  790\n",
      "real is  903\n",
      "tot is  1693\n",
      "Ratio is  46.66272888363851\n",
      "len is  1693\n",
      "***********Day is  Wednesday\n",
      "fake is  836\n",
      "real is  758\n",
      "tot is  1594\n",
      "Ratio is  52.446675031367626\n",
      "len is  1594\n",
      "***********Day is  Tuesday\n",
      "fake is  150\n",
      "real is  161\n",
      "tot is  311\n",
      "Ratio is  48.231511254019296\n",
      "len is  311\n",
      "***********Day is  Thursday\n",
      "fake is  129\n",
      "real is  128\n",
      "tot is  257\n",
      "Ratio is  50.19455252918288\n",
      "len is  257\n",
      "***********Day is  Saturday\n",
      "fake is  137\n",
      "real is  117\n",
      "tot is  254\n",
      "Ratio is  53.937007874015755\n",
      "len is  254\n",
      "***********Day is  Monday\n",
      "fake is  169\n",
      "real is  166\n",
      "tot is  335\n",
      "Ratio is  50.44776119402985\n",
      "len is  335\n",
      "***********Day is  Wednesday\n",
      "fake is  159\n",
      "real is  141\n",
      "tot is  300\n",
      "Ratio is  53.0\n",
      "len is  300\n",
      "***********Day is  Sunday\n",
      "fake is  138\n",
      "real is  153\n",
      "tot is  291\n",
      "Ratio is  47.42268041237113\n",
      "len is  291\n",
      "***********Day is  Friday\n",
      "fake is  118\n",
      "real is  134\n",
      "tot is  252\n",
      "Ratio is  46.82539682539682\n",
      "len is  252\n",
      "***********Day is  Friday\n",
      "fake is  661\n",
      "real is  638\n",
      "tot is  1299\n",
      "Ratio is  50.88529638183218\n",
      "len is  1299\n",
      "***********Day is  Wednesday\n",
      "fake is  728\n",
      "real is  637\n",
      "tot is  1365\n",
      "Ratio is  53.333333333333336\n",
      "len is  1365\n",
      "***********Day is  Tuesday\n",
      "fake is  754\n",
      "real is  734\n",
      "tot is  1488\n",
      "Ratio is  50.67204301075269\n",
      "len is  1488\n",
      "***********Day is  Sunday\n",
      "fake is  704\n",
      "real is  807\n",
      "tot is  1511\n",
      "Ratio is  46.591661151555265\n",
      "len is  1511\n",
      "***********Day is  Saturday\n",
      "fake is  638\n",
      "real is  660\n",
      "tot is  1298\n",
      "Ratio is  49.152542372881356\n",
      "len is  1298\n",
      "***********Day is  Thursday\n",
      "fake is  681\n",
      "real is  645\n",
      "tot is  1326\n",
      "Ratio is  51.35746606334841\n",
      "len is  1326\n",
      "***********Day is  Monday\n",
      "fake is  834\n",
      "real is  879\n",
      "tot is  1713\n",
      "Ratio is  48.68651488616462\n",
      "len is  1713\n"
     ]
    }
   ],
   "source": [
    "# reviewer_centric features #\n",
    "#train.user_id.value_counts()\n",
    "\n",
    "train['date.1'] = pd.to_datetime(train['date.1'])\n",
    "train['day_of_week'] = train['date.1'].dt.day_name()\n",
    "#train.day_of_week.value_counts()[:10]]\n",
    "dofw = train.day_of_week.unique()\n",
    "for day in range(len(dofw)):\n",
    "        label=train['label'][train.day_of_week == dofw[day]]\n",
    "        fake=len(label[label==1])\n",
    "        real=len(label[label==0])\n",
    "        \n",
    "        print(\"***********Day is \",dofw[day])\n",
    "        print(\"fake is \",fake)\n",
    "        print(\"real is \",real)\n",
    "        print(\"tot is \",fake+real)\n",
    "        print(\"Ratio is \",100*(fake/(fake+real)))\n",
    "        print(\"len is \",len(label))\n",
    "\n",
    "train['user_id_no_of_review'] = train.groupby('user_id')['user_id'].transform('size')\n",
    "train['user_id_ave_rating'] = train.groupby('user_id')['rating'].transform('mean')\n",
    "#train['user_id_std_rating'] = train.groupby('user_id')['rating'].transform('std')\n",
    "train['user_id_ave_no_words'] = train.groupby('user_id')['word_count'].transform('mean')\n",
    "train['user_id_max_review_a_day'] = train['user_id_no_of_review']\n",
    "grouped = train.groupby('user_id')\n",
    "\n",
    "for name,group in grouped:\n",
    "    #print(name)\n",
    "    #print(group)\n",
    "    df2 = group.groupby('date').size().max()\n",
    "    #print(df2)\n",
    "    train.loc[train.user_id == name,'user_id_max_review_a_day'] = df2\n",
    "    #group['user_id_max_review_a_day']=df2\n",
    "\n",
    "#train[:][train.user_id==9236].sort_index(axis = 0)\n",
    "    \n",
    "valid['date.1'] = pd.to_datetime(valid['date.1'])\n",
    "valid['day_of_week'] = valid['date.1'].dt.day_name()\n",
    "#valid.day_of_week.value_counts()[:10]]\n",
    "dofw = valid.day_of_week.unique()\n",
    "for day in range(len(dofw)):\n",
    "        label=valid['label'][valid.day_of_week == dofw[day]]\n",
    "        fake=len(label[label==1])\n",
    "        real=len(label[label==0])\n",
    "        \n",
    "        print(\"***********Day is \",dofw[day])\n",
    "        print(\"fake is \",fake)\n",
    "        print(\"real is \",real)\n",
    "        print(\"tot is \",fake+real)\n",
    "        print(\"Ratio is \",100*(fake/(fake+real)))\n",
    "        print(\"len is \",len(label))\n",
    "\n",
    "valid['user_id_no_of_review'] = valid.groupby('user_id')['user_id'].transform('size')\n",
    "valid['user_id_ave_rating'] = valid.groupby('user_id')['rating'].transform('mean')\n",
    "#valid['user_id_std_rating'] = valid.groupby('user_id')['rating'].transform('std')\n",
    "valid['user_id_ave_no_words'] = valid.groupby('user_id')['word_count'].transform('mean')\n",
    "valid['user_id_max_review_a_day'] = valid['user_id_no_of_review']\n",
    "grouped = valid.groupby('user_id')\n",
    "\n",
    "for name,group in grouped:\n",
    "    #print(name)\n",
    "    #print(group)\n",
    "    df2 = group.groupby('date').size().max()\n",
    "    #print(df2)\n",
    "    valid.loc[valid.user_id == name,'user_id_max_review_a_day'] = df2\n",
    "    #group['user_id_max_review_a_day']=df2\n",
    "\n",
    "#valid[:][valid.user_id==9236].sort_index(axis = 0) \n",
    "\n",
    "test['date.1'] = pd.to_datetime(test['date.1'])\n",
    "test['day_of_week'] = test['date.1'].dt.day_name()\n",
    "#test.day_of_week.value_counts()[:10]]\n",
    "dofw = test.day_of_week.unique()\n",
    "for day in range(len(dofw)):\n",
    "        label=test['label'][test.day_of_week == dofw[day]]\n",
    "        fake=len(label[label==1])\n",
    "        real=len(label[label==0])\n",
    "        \n",
    "        print(\"***********Day is \",dofw[day])\n",
    "        print(\"fake is \",fake)\n",
    "        print(\"real is \",real)\n",
    "        print(\"tot is \",fake+real)\n",
    "        print(\"Ratio is \",100*(fake/(fake+real)))\n",
    "        print(\"len is \",len(label))\n",
    "\n",
    "test['user_id_no_of_review'] = test.groupby('user_id')['user_id'].transform('size')\n",
    "test['user_id_ave_rating'] = test.groupby('user_id')['rating'].transform('mean')\n",
    "#test['user_id_std_rating'] = test.groupby('user_id')['rating'].transform('std')\n",
    "test['user_id_ave_no_words'] = test.groupby('user_id')['word_count'].transform('mean')\n",
    "test['user_id_max_review_a_day'] = test['user_id_no_of_review']\n",
    "grouped = test.groupby('user_id')\n",
    "\n",
    "for name,group in grouped:\n",
    "    #print(name)\n",
    "    #print(group)\n",
    "    df2 = group.groupby('date').size().max()\n",
    "    #print(df2)\n",
    "    test.loc[test.user_id == name,'user_id_max_review_a_day'] = df2\n",
    "    #group['user_id_max_review_a_day']=df2\n",
    "\n",
    "#test[:][test.user_id==9236].sort_index(axis = 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_centric features #\n",
    "#train.prod_id.value_counts()\n",
    "\n",
    "train['prod_id_no_of_review'] = train.groupby('prod_id')['prod_id'].transform('size') + 1000.\n",
    "train['prod_id_ave_rating'] = train.groupby('prod_id')['rating'].transform('mean') + 1000.\n",
    "#train['prod_id_std_rating'] = train.groupby('prod_id')['rating'].transform('std') + 1000.\n",
    "train['prod_id_ave_no_words'] = train.groupby('prod_id')['word_count'].transform('mean') + 1000.\n",
    "train['prod_id_max_review_a_day'] = train['prod_id_no_of_review'] + 1000.\n",
    "grouped = train.groupby('prod_id')\n",
    "\n",
    "for name,group in grouped:\n",
    "    #print(name)\n",
    "    #print(group)\n",
    "    df2 = group.groupby('date').size().max()\n",
    "    #print(df2)\n",
    "    train.loc[train.prod_id == name,'prod_id_max_review_a_day'] = df2 + 1000.\n",
    "    #group['prod_id_max_review_a_day']=df2\n",
    "\n",
    "#train[:][train.prod_id==9236].sort_index(axis = 0)\n",
    "\n",
    "valid['prod_id_no_of_review'] = valid.groupby('prod_id')['prod_id'].transform('size') + 1000.\n",
    "valid['prod_id_ave_rating'] = valid.groupby('prod_id')['rating'].transform('mean') + 1000.\n",
    "#valid['prod_id_std_rating'] = valid.groupby('prod_id')['rating'].transform('std') + 1000.\n",
    "valid['prod_id_ave_no_words'] = valid.groupby('prod_id')['word_count'].transform('mean') + 1000.\n",
    "valid['prod_id_max_review_a_day'] = valid['prod_id_no_of_review']\n",
    "grouped = valid.groupby('prod_id')\n",
    "\n",
    "for name,group in grouped:\n",
    "    #print(name)\n",
    "    #print(group)\n",
    "    df2 = group.groupby('date').size().max()\n",
    "    #print(df2)\n",
    "    valid.loc[valid.prod_id == name,'prod_id_max_review_a_day'] = df2 + 1000.\n",
    "    #group['prod_id_max_review_a_day']=df2\n",
    "\n",
    "#valid[:][valid.prod_id==9236].sort_index(axis = 0)  \n",
    "\n",
    "test['prod_id_no_of_review'] = test.groupby('prod_id')['prod_id'].transform('size') + 1000.\n",
    "test['prod_id_ave_rating'] = test.groupby('prod_id')['rating'].transform('mean') + 1000.\n",
    "#test['prod_id_std_rating'] = test.groupby('prod_id')['rating'].transform('std') + 1000.\n",
    "test['prod_id_ave_no_words'] = test.groupby('prod_id')['word_count'].transform('mean') + 1000.\n",
    "test['prod_id_max_review_a_day'] = test['prod_id_no_of_review']\n",
    "grouped = test.groupby('prod_id')\n",
    "\n",
    "for name,group in grouped:\n",
    "    #print(name)\n",
    "    #print(group)\n",
    "    df2 = group.groupby('date').size().max()\n",
    "    #print(df2)\n",
    "    test.loc[test.prod_id == name,'prod_id_max_review_a_day'] = df2 + 1000.\n",
    "    #group['prod_id_max_review_a_day']=df2\n",
    "\n",
    "#test[:][test.prod_id==9236].sort_index(axis = 0)  \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['total']=train.user_id.astype(str) + ' '+ train.prod_id.astype(str) + ' ' + train.rating.astype(str) + ' '+ \\\n",
    "train.user_id_no_of_review.astype(str) + ' ' + train.user_id_ave_rating.astype(str) + ' '+\\\n",
    "train.user_id_ave_no_words.astype(str) + ' ' + train.user_id_max_review_a_day.astype(str) + ' '+\\\n",
    "train.prod_id_no_of_review.astype(str) + ' ' + train.prod_id_ave_rating.astype(str) + ' '+\\\n",
    "train.prod_id_ave_no_words.astype(str) + ' ' + train.prod_id_max_review_a_day.astype(str) + ' '+\\\n",
    "train.day_of_week.astype(str) + ' '+train.review  \n",
    "\n",
    "#train['total']=train.user_id.astype(str) + ' '+ train.prod_id.astype(str) + ' ' + train.rating.astype(str) + ' '+  train.review\n",
    "#test['total']=test.user_id.astype(str) + ' '+ test.prod_id.astype(str) + ' ' + test.rating.astype(str) + ' '+test.review\n",
    "\n",
    "valid['total']=valid.user_id.astype(str) + ' '+ valid.prod_id.astype(str) + ' ' + valid.rating.astype(str) + ' '+ \\\n",
    "valid.user_id_no_of_review.astype(str) + ' ' + valid.user_id_ave_rating.astype(str) + ' '+\\\n",
    "valid.user_id_ave_no_words.astype(str) + ' ' + valid.user_id_max_review_a_day.astype(str) + ' '+\\\n",
    "valid.prod_id_no_of_review.astype(str) + ' ' + valid.prod_id_ave_rating.astype(str) + ' '+\\\n",
    "valid.prod_id_ave_no_words.astype(str) + ' ' + valid.prod_id_max_review_a_day.astype(str) + ' '+\\\n",
    "valid.day_of_week.astype(str) + ' '+ valid.review  \n",
    "\n",
    "test['total']=test.user_id.astype(str) + ' '+ test.prod_id.astype(str) + ' ' + test.rating.astype(str) + ' '+ \\\n",
    "test.user_id_no_of_review.astype(str) + ' ' + test.user_id_ave_rating.astype(str) + ' '+\\\n",
    "test.user_id_ave_no_words.astype(str) + ' ' + test.user_id_max_review_a_day.astype(str) + ' '+\\\n",
    "test.prod_id_no_of_review.astype(str) + ' ' + test.prod_id_ave_rating.astype(str) + ' '+\\\n",
    "test.prod_id_ave_no_words.astype(str) + ' ' + test.prod_id_max_review_a_day.astype(str) + ' '+\\\n",
    "test.day_of_week.astype(str) + ' '+ test.review  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Turker\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import SnowballStemmer\n",
    "porter = SnowballStemmer(\"english\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy import sparse\n",
    "def apply_PCA(feature_vector_train, feature_vector_valid, feature_vector_test, n_components = 300):\n",
    "    # applys PCA\n",
    "    pca = PCA(n_components)\n",
    "    #pca = PCA(0.95)\n",
    "    pca.fit(feature_vector_train.toarray())\n",
    "    xtrain = pca.transform(feature_vector_train.toarray())  \n",
    "    xvalid = pca.transform(feature_vector_valid.toarray())       \n",
    "    xtest  = pca.transform(feature_vector_test.toarray()) \n",
    "    \n",
    "    feature_vector_train = sparse.csr_matrix(xtrain)\n",
    "    feature_vector_valid = sparse.csr_matrix(xvalid)\n",
    "    feature_vector_test  = sparse.csr_matrix(xtest)    \n",
    "    \n",
    "    return [feature_vector_train, feature_vector_valid, feature_vector_test]\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "def apply_LDA(feature_vector_train, feature_vector_valid, feature_vector_test, n_components = 300):\n",
    "    # applys lda\n",
    "    lda = LatentDirichletAllocation(n_components, random_state=0)\n",
    "    #lda = lda(0.95)\n",
    "    lda.fit(feature_vector_train.toarray())\n",
    "    xtrain = lda.transform(feature_vector_train.toarray())  \n",
    "    xvalid = lda.transform(feature_vector_valid.toarray())       \n",
    "    xtest  = lda.transform(feature_vector_test.toarray()) \n",
    "    \n",
    "    feature_vector_train = sparse.csr_matrix(xtrain)\n",
    "    feature_vector_valid = sparse.csr_matrix(xvalid)\n",
    "    feature_vector_test  = sparse.csr_matrix(xtest)    \n",
    "    \n",
    "    return [feature_vector_train, feature_vector_valid, feature_vector_test]\n",
    "\n",
    "\n",
    "def scale_feature(col):\n",
    "    x = col.values.astype(float)\n",
    "    # Create a minimum and maximum processor object\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()    \n",
    "    # Create an object to transform the data to fit minmax processor\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    col = x_scaled\n",
    "    return col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "def add_features(feature_vector_train, feature_vector_valid, feature_vector_test):\n",
    "    \n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['user_id_no_of_review'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['user_id_ave_rating'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['user_id_ave_no_words'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['user_id_max_review_a_day'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['prod_id_no_of_review'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['prod_id_ave_rating'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['prod_id_ave_no_words'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['prod_id_max_review_a_day'])[:,None]))    \n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['char_count'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['word_count'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['word_density'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['punctuation_count'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['title_word_count'])[:,None]))\n",
    "    feature_vector_train = hstack((feature_vector_train,np.array(train['upper_case_word_count'])[:,None]))\n",
    "    \n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['user_id_no_of_review'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['user_id_ave_rating'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['user_id_ave_no_words'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['user_id_max_review_a_day'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['prod_id_no_of_review'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['prod_id_ave_rating'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['prod_id_ave_no_words'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['prod_id_max_review_a_day'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['char_count'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['word_count'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['word_density'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['punctuation_count'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['title_word_count'])[:,None]))\n",
    "    feature_vector_valid = hstack((feature_vector_valid,np.array(valid['upper_case_word_count'])[:,None]))\n",
    "    \n",
    "    \n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['user_id_no_of_review'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['user_id_ave_rating'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['user_id_ave_no_words'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['user_id_max_review_a_day'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['prod_id_no_of_review'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['prod_id_ave_rating'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['prod_id_ave_no_words'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['prod_id_max_review_a_day'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['char_count'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['word_count'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['word_density'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['punctuation_count'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['title_word_count'])[:,None]))\n",
    "    feature_vector_test = hstack((feature_vector_test,np.array(test['upper_case_word_count'])[:,None]))\n",
    "    \n",
    "    return [feature_vector_train, feature_vector_valid, feature_vector_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "#count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, stop_words='english')\n",
    "#count_vect.fit(train.total)\n",
    "#\n",
    "#x_train_count =  count_vect.transform(train.total)\n",
    "#x_valid_count =  count_vect.transform(valid.total)\n",
    "#x_test_count =  count_vect.transform(test.total)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, stop_words='english')\n",
    "count_vect.fit(train.review)\n",
    "\n",
    "x_train_count =  count_vect.transform(train.review)\n",
    "x_valid_count =  count_vect.transform(valid.review)\n",
    "x_test_count =  count_vect.transform(test.review)\n",
    "\n",
    "count_vect_LR = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, stop_words='english')\n",
    "count_vect_LR.fit(train.total)\n",
    "\n",
    "x_train_count_LR =  count_vect_LR.transform(train.total)\n",
    "x_valid_count_LR =  count_vect_LR.transform(valid.total)\n",
    "x_test_count_LR =  count_vect_LR.transform(test.total)\n",
    "\n",
    "[x_train_count_LR, x_valid_count_LR, x_test_count_LR] = apply_PCA(x_train_count, x_valid_count, x_test_count)\n",
    "[x_train_count_LR, x_valid_count_LR, x_test_count_LR] = add_features(x_train_count_LR, x_valid_count_LR, x_test_count_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def train_model_all(classifier, feature_vector_train, label_train, feature_vector_valid, label_valid, feature_vector_test, label_test, is_neural_net=False):\n",
    "#    scoring = {'acc': 'accuracy',\n",
    "#           'prec_macro': 'precision_macro',\n",
    "#           'rec_micro': 'recall_macro'}\n",
    "#    scores = cross_val_score(classifier,feature_vector_train, label_train, scoring=scoring, cv=5)\n",
    "    scores = cross_val_score(classifier,feature_vector_train, label_train, cv=5)\n",
    "    print(scores)\n",
    "    # fit the train dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label_train)\n",
    "    # predict the labels on train and test dataset\n",
    "    probs_train = classifier.predict_proba(feature_vector_train)\n",
    "    probs_valid = classifier.predict_proba(feature_vector_valid)\n",
    "    probs_test = classifier.predict_proba(feature_vector_test)\n",
    "    #print(\"Probs is \",probs[:,1])\n",
    "    # predict the labels on validation dataset    \n",
    "    predictions_train = classifier.predict(feature_vector_train)\n",
    "    predictions_valid = classifier.predict(feature_vector_valid)\n",
    "    predictions_test = classifier.predict(feature_vector_test)\n",
    "    return [predictions_train, np.mean(scores), probs_train , predictions_valid,metrics.accuracy_score(predictions_valid, label_valid),probs_valid,  predictions_test,metrics.accuracy_score(predictions_test, label_test),probs_test]\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,label_valid, is_neural_net=False):\n",
    "    # fit the train dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    probs = classifier.predict_proba(feature_vector_valid)\n",
    "    #print(\"Probs is \",probs[:,1])\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return [predictions,metrics.accuracy_score(predictions, label_valid),probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create first pipeline for base without reducing features.\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#\n",
    "#pipe = Pipeline([('classifier' , linear_model.LogisticRegression())])\n",
    "## pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "##lr = linear_model.LogisticRegression()\n",
    "#\n",
    "## Create param grid.\n",
    "##solver_options = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "#param_grid = [\n",
    "#    {'classifier' : [linear_model.LogisticRegression()],\n",
    "#     'classifier__penalty' : ['L1','l2'],\n",
    "#    'classifier__C' : np.logspace(1, 2, 3),\n",
    "#    'classifier__solver' : ['liblinear']}]\n",
    "#\n",
    "## Create grid search object\n",
    "#\n",
    "#clf = GridSearchCV(pipe, param_grid = param_grid,verbose=True, cv=5, n_jobs=-1)\n",
    "#\n",
    "## Fit on data\n",
    "#\n",
    "#best_clf = clf.fit(x_train_tfidf_ngram_chars, train.label)\n",
    "#\n",
    "#pd.DataFrame.from_dict(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Turker\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61814915 0.61096137 0.61196043 0.61106115 0.62365108]\n",
      "LR, Count Vectors => acc train:  0.6151566347999767 \tacc valid:  0.6215 \tacc test:  0.6142\n",
      "************ Logistic Regression Test***********\n",
      "LR, Count Vectors:  0.6142\n",
      "Auc ROC is  0.6485118\n",
      "Auc PRC is  0.6044357428407179\n",
      "F1 score is 0.6435698447893571\n",
      "Log loss is  0.8362353336338563\n",
      "************ Logistic Regression Valid***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Turker\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.6215\n",
      "Auc ROC is  0.6599809999999999\n",
      "Auc PRC is  0.6286268897010261\n",
      "F1 score is 0.6447677146879399\n",
      "Log loss is  0.8264354150509815\n",
      "************ Logistic Regression Train***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Turker\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.8553577849694355\n",
      "Auc ROC is  0.9321586474252974\n",
      "Auc PRC is  0.9261263422332073\n",
      "F1 score is 0.8617341239150984\n",
      "Log loss is  0.35870406860433984\n"
     ]
    }
   ],
   "source": [
    "train_y=train.label\n",
    "test_y=test.label\n",
    "valid_y=valid.label\n",
    "\n",
    "train_x=x_train_count\n",
    "test_x=x_test_count\n",
    "valid_x=x_valid_count\n",
    "\n",
    "[pred_train,acc_train,prob_train, pred_valid,acc_valid,prob_valid, pred_test,acc_test,prob_test] =\\\n",
    "train_model_all(linear_model.LogisticRegression(), train_x, train_y, valid_x, valid_y, test_x, test_y)\n",
    "\n",
    "print (\"LR, Count Vectors => acc train: \", acc_train, \"\\tacc valid: \", acc_valid, \"\\tacc test: \", acc_test)\n",
    "print(\"************ Logistic Regression Test***********\")\n",
    "\n",
    "[predictions_ll,accuracy_ll,probs_ll] = train_model(linear_model.LogisticRegression(), train_x, train_y, test_x, test_y)\n",
    "print (\"LR, Count Vectors: \", accuracy_ll)\n",
    "fpr_ll, tpr_ll, thresholds = metrics.roc_curve(test_y, probs_ll[:,1])\n",
    "auc_ll = metrics.roc_auc_score(test_y, probs_ll[:,1])\n",
    "print(\"Auc ROC is \",auc_ll)\n",
    "precision_ll, recall_ll, thresholds = metrics.precision_recall_curve(test_y, probs_ll[:,1])\n",
    "auc_ll = metrics.auc(recall_ll, precision_ll)\n",
    "print(\"Auc PRC is \",auc_ll)\n",
    "f1_ll = metrics.f1_score(test_y, predictions_ll)\n",
    "print(\"F1 score is\",f1_ll)\n",
    "print(\"Log loss is \",metrics.log_loss(test_y,probs_ll[:,1]))\n",
    "\n",
    "print(\"************ Logistic Regression Valid***********\")\n",
    "\n",
    "[predictions_ll,accuracy_ll,probs_ll] = train_model(linear_model.LogisticRegression(), train_x, train_y, valid_x, valid_y)\n",
    "print (\"LR, Count Vectors: \", accuracy_ll)\n",
    "fpr_ll, tpr_ll, thresholds = metrics.roc_curve(valid_y, probs_ll[:,1])\n",
    "auc_ll = metrics.roc_auc_score(valid_y, probs_ll[:,1])\n",
    "print(\"Auc ROC is \",auc_ll)\n",
    "precision_ll, recall_ll, thresholds = metrics.precision_recall_curve(valid_y, probs_ll[:,1])\n",
    "auc_ll = metrics.auc(recall_ll, precision_ll)\n",
    "print(\"Auc PRC is \",auc_ll)\n",
    "f1_ll = metrics.f1_score(valid_y, predictions_ll)\n",
    "print(\"F1 score is\",f1_ll)\n",
    "print(\"Log loss is \",metrics.log_loss(valid_y,probs_ll[:,1]))\n",
    "\n",
    "print(\"************ Logistic Regression Train***********\")\n",
    "\n",
    "[predictions_ll,accuracy_ll,probs_ll] = train_model(linear_model.LogisticRegression(), train_x, train_y, train_x, train_y)\n",
    "print (\"LR, Count Vectors: \", accuracy_ll)\n",
    "fpr_ll, tpr_ll, thresholds = metrics.roc_curve(train_y, probs_ll[:,1])\n",
    "auc_ll = metrics.roc_auc_score(train_y, probs_ll[:,1])\n",
    "print(\"Auc ROC is \",auc_ll)\n",
    "precision_ll, recall_ll, thresholds = metrics.precision_recall_curve(train_y, probs_ll[:,1])\n",
    "auc_ll = metrics.auc(recall_ll, precision_ll)\n",
    "print(\"Auc PRC is \",auc_ll)\n",
    "f1_ll = metrics.f1_score(train_y, predictions_ll)\n",
    "print(\"F1 score is\",f1_ll)\n",
    "print(\"Log loss is \",metrics.log_loss(train_y,probs_ll[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Logistic Regression Test + Features ***********\n",
      "[0.68733154 0.655885   0.66636691 0.66771583 0.68255396]\n",
      "LR, Count Vectors => acc train:  0.6719706445086518 \tacc valid:  0.654 \tacc test:  0.6537\n",
      "LR, Count Vectors:  0.6537\n",
      "Auc ROC is  0.71780496\n",
      "Auc PRC is  0.7060661580298724\n",
      "F1 score is 0.6850386539336062\n",
      "Log loss is  0.6143693375427698\n",
      "recall_score score is 0.7532\n",
      "precision_score score is 0.6281901584653878\n",
      "************ Logistic Regression Valid***********\n",
      "LR, Count Vectors:  0.654\n",
      "Auc ROC is  0.7086490000000001\n",
      "Auc PRC is  0.6903775102407947\n",
      "F1 score is 0.6828597616865261\n",
      "Log loss is  0.6222454156814841\n",
      "************ Logistic Regression Train***********\n",
      "LR, Count Vectors:  0.6947141316073355\n",
      "Auc ROC is  0.7649571804150551\n",
      "Auc PRC is  0.7569854627997736\n",
      "F1 score is 0.7084978540772533\n",
      "Log loss is  0.5809369056318081\n"
     ]
    }
   ],
   "source": [
    "train_x=x_train_count_LR\n",
    "test_x=x_test_count_LR\n",
    "valid_x=x_valid_count_LR\n",
    "\n",
    "\n",
    "print(\"************ Logistic Regression Test + Features ***********\")\n",
    "[pred_train,acc_train,prob_train, pred_valid,acc_valid,prob_valid, pred_test,acc_test,prob_test] =\\\n",
    "train_model_all(linear_model.LogisticRegression(C=0.01, solver='liblinear'), train_x, train_y, valid_x, valid_y, test_x, test_y)\n",
    "\n",
    "print (\"LR, Count Vectors => acc train: \", acc_train, \"\\tacc valid: \", acc_valid, \"\\tacc test: \", acc_test)\n",
    "[predictions_ll,accuracy_ll,probs_ll] = train_model(linear_model.LogisticRegression(C=0.01, solver='liblinear'), train_x, train_y, test_x, test_y)\n",
    "print (\"LR, Count Vectors: \", accuracy_ll)\n",
    "fpr_ll, tpr_ll, thresholds = metrics.roc_curve(test_y, probs_ll[:,1])\n",
    "auc_ll = metrics.roc_auc_score(test_y, probs_ll[:,1])\n",
    "print(\"Auc ROC is \",auc_ll)\n",
    "precision_ll, recall_ll, thresholds = metrics.precision_recall_curve(test_y, probs_ll[:,1])\n",
    "auc_ll = metrics.auc(recall_ll, precision_ll)\n",
    "print(\"Auc PRC is \",auc_ll)\n",
    "f1_ll = metrics.f1_score(test_y, predictions_ll)\n",
    "print(\"F1 score is\",f1_ll)\n",
    "print(\"Log loss is \",metrics.log_loss(test_y,probs_ll[:,1]))\n",
    "\n",
    "recall_score = metrics.recall_score(test_y, predictions_ll)\n",
    "print(\"recall_score score is\",recall_score)\n",
    "precision_score = metrics.precision_score(test_y, predictions_ll)\n",
    "print(\"precision_score score is\",precision_score)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['prob'] = probs_ll[:,1]\n",
    "results['predictions'] = predictions_ll\n",
    "results['labels'] = test_y\n",
    "results.to_csv(('LR.csv'))\n",
    "\n",
    "print(\"************ Logistic Regression Valid***********\")\n",
    "\n",
    "[predictions_ll,accuracy_ll,probs_ll] = train_model(linear_model.LogisticRegression(C=0.01, solver='liblinear'), train_x, train_y, valid_x, valid_y)\n",
    "print (\"LR, Count Vectors: \", accuracy_ll)\n",
    "fpr_ll, tpr_ll, thresholds = metrics.roc_curve(valid_y, probs_ll[:,1])\n",
    "auc_ll = metrics.roc_auc_score(valid_y, probs_ll[:,1])\n",
    "print(\"Auc ROC is \",auc_ll)\n",
    "precision_ll, recall_ll, thresholds = metrics.precision_recall_curve(valid_y, probs_ll[:,1])\n",
    "auc_ll = metrics.auc(recall_ll, precision_ll)\n",
    "print(\"Auc PRC is \",auc_ll)\n",
    "f1_ll = metrics.f1_score(valid_y, predictions_ll)\n",
    "print(\"F1 score is\",f1_ll)\n",
    "print(\"Log loss is \",metrics.log_loss(valid_y,probs_ll[:,1]))\n",
    "\n",
    "print(\"************ Logistic Regression Train***********\")\n",
    "\n",
    "[predictions_ll,accuracy_ll,probs_ll] = train_model(linear_model.LogisticRegression(C=0.01, solver='liblinear'), train_x, train_y, train_x, train_y)\n",
    "print (\"LR, Count Vectors: \", accuracy_ll)\n",
    "fpr_ll, tpr_ll, thresholds = metrics.roc_curve(train_y, probs_ll[:,1])\n",
    "auc_ll = metrics.roc_auc_score(train_y, probs_ll[:,1])\n",
    "print(\"Auc ROC is \",auc_ll)\n",
    "precision_ll, recall_ll, thresholds = metrics.precision_recall_curve(train_y, probs_ll[:,1])\n",
    "auc_ll = metrics.auc(recall_ll, precision_ll)\n",
    "print(\"Auc PRC is \",auc_ll)\n",
    "f1_ll = metrics.f1_score(train_y, predictions_ll)\n",
    "print(\"F1 score is\",f1_ll)\n",
    "print(\"Log loss is \",metrics.log_loss(train_y,probs_ll[:,1]))\n",
    "## in 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features=['user_id','prod_id','rating']\n",
    "#test=np.array(test[features])\n",
    "#valid_y=test.label\n",
    "#[predictions,accuracy,probs] = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "##test=np.concatenate((test,np.resize(probs,(len(probs[:,1]),1))),1)\n",
    "#\n",
    "#train=np.array(train[features])\n",
    "#valid_y=train.label\n",
    "#[predictions,accuracy,probs] = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xtrain_tfidf_ngram)\n",
    "#train=np.concatenate((train,np.resize(probs,(len(probs[:,1]),1))),1)\n",
    "\n",
    "## PLOTS\n",
    "#import matplotlib.pyplot as plt\n",
    "## plot the roc curve for the model\n",
    "#plt.plot(fpr, tpr, linestyle='--', label='NB')\n",
    "#plt.plot(fpr_ll, tpr_ll, linestyle='-', label='Logistic Regression')\n",
    "##pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "## axis labels\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "## show the legend\n",
    "#plt.legend()\n",
    "## show the plot\n",
    "#plt.show()\n",
    "#\n",
    "#import matplotlib.pyplot as plt\n",
    "## plot the roc curve for the model\n",
    "#plt.plot(recall, precision, marker='.',label='NB')\n",
    "#plt.plot(recall_ll, precision_ll, marker='x',label='Logistic Regression')\n",
    "##pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "## axis labels\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "## show the legend\n",
    "#plt.legend()\n",
    "## show the plot\n",
    "#plt.show()\n",
    "#\n",
    "#[y,x]=calibration.calibration_curve(valid_y,probs[:,1])\n",
    "#[y_ll,x_ll]=calibration.calibration_curve(valid_y,probs_ll[:,1])\n",
    "#plt.plot(x,y, marker='o', linewidth=1, label='NB')\n",
    "#plt.plot(x_ll,y_ll, marker='x', linewidth=1, label='Logistic Regression')\n",
    "#x1=np.linspace(0,1,11)\n",
    "#y1=x1\n",
    "#plt.plot(x1,y1,linestyle='--')\n",
    "#plt.legend()\n",
    "#\n",
    "#plt.hist(probs[:,1],bins=10,histtype='step',label='NB')\n",
    "#plt.hist(probs_ll[:,1],bins=10,histtype='step',label='Logistic Regression')\n",
    "#plt.xlabel('Prob')\n",
    "#plt.ylabel('Count')\n",
    "#plt.legend()\n",
    "#\n",
    "#y_test=test.label\n",
    "#y_pred=predictions\n",
    "#class_names=[\"True\",\"False\"]\n",
    "#cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "#fig, ax = plt.subplots()\n",
    "#tick_marks = np.arange(len(class_names))\n",
    "#plt.xticks(tick_marks, class_names)\n",
    "#plt.yticks(tick_marks, class_names)\n",
    "#sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"viridis\" ,fmt='g')\n",
    "#ax.xaxis.set_label_position(\"top\")\n",
    "#plt.tight_layout()\n",
    "#plt.title('Confusion matrix - NB', y=1.1)\n",
    "#plt.ylabel('Actual label')\n",
    "#plt.xlabel('Predicted label')\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "#print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "#\n",
    "#\n",
    "#y_test=test.label\n",
    "#y_pred=predictions_ll\n",
    "#class_names=[\"True\",\"False\"]\n",
    "#cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "#fig, ax = plt.subplots()\n",
    "#tick_marks = np.arange(len(class_names))\n",
    "#plt.xticks(tick_marks, class_names)\n",
    "#plt.yticks(tick_marks, class_names)\n",
    "#sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"viridis\" ,fmt='g')\n",
    "#ax.xaxis.set_label_position(\"top\")\n",
    "#plt.tight_layout()\n",
    "#plt.title('Confusion matrix - Logistic Regression', y=1.1)\n",
    "#plt.ylabel('Actual label')\n",
    "#plt.xlabel('Predicted label')\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "#print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "#import pandas as pd\n",
    "#from sklearn import preprocessing\n",
    "#x = train[['rating']].values.astype(float)\n",
    "## Create a minimum and maximum processor object\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#\n",
    "## Create an object to transform the data to fit minmax processor\n",
    "#x_scaled = min_max_scaler.fit_transform(x)\n",
    "#train.rating=x_scaled\n",
    "\n",
    "# Trainind data set results\n",
    "#train_y=train.label\n",
    "#valid_y=train.label\n",
    "#\n",
    "#train_x=xtrain_tfidf_ngram\n",
    "#test_x=xtrain_tfidf_ngram\n",
    "#\n",
    "#print(\"************ Naive Bayes***********\")\n",
    "#[predictions,accuracy,probs] = train_model(naive_bayes.MultinomialNB(), train_x, train_y, test_x)\n",
    "#print (\"NB, Count Vectors: \", accuracy)\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(valid_y, probs[:,1])\n",
    "#auc = metrics.roc_auc_score(valid_y, probs[:,1])\n",
    "#print(\"Auc ROC is \",auc)\n",
    "#precision, recall, thresholds = metrics.precision_recall_curve(valid_y, probs[:,1])\n",
    "#auc = metrics.auc(recall, precision)\n",
    "#print(\"Auc PRC is \",auc)\n",
    "#f1 = metrics.f1_score(valid_y, predictions)\n",
    "#print(\"F1 score is\",f1)\n",
    "#print(\"Log loss is \",metrics.log_loss(valid_y,probs[:,1]))\n",
    "#\n",
    "#\n",
    "#\n",
    "#print(\"************ Logistic Regression***********\")\n",
    "#\n",
    "#[predictions_ll,accuracy_ll,probs_ll] = train_model(linear_model.LogisticRegression(), train_x, train_y, test_x)\n",
    "#print (\"LR, Count Vectors: \", accuracy_ll)\n",
    "#fpr_ll, tpr_ll, thresholds = metrics.roc_curve(valid_y, probs_ll[:,1])\n",
    "#auc_ll = metrics.roc_auc_score(valid_y, probs_ll[:,1])\n",
    "#print(\"Auc ROC is \",auc_ll)\n",
    "#precision_ll, recall_ll, thresholds = metrics.precision_recall_curve(valid_y, probs_ll[:,1])\n",
    "#auc_ll = metrics.auc(recall_ll, precision_ll)\n",
    "#print(\"Auc PRC is \",auc_ll)\n",
    "#f1_ll = metrics.f1_score(valid_y, predictions_ll)\n",
    "#print(\"F1 score is\",f1_ll)\n",
    "#print(\"Log loss is \",metrics.log_loss(valid_y,probs_ll[:,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
